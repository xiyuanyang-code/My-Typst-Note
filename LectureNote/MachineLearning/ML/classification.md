# 机器学习课程 — 第三节课大纲（分类 Classification, 90分钟）

## 1. 引入与动机（10分钟）
- 为什么需要分类？
  - 区分类别而不是连续数值
  - 图像识别（猫 vs 狗）
  - 邮件过滤（垃圾邮件 vs 正常邮件）
  - 医学诊断（健康 vs 疾病）
- 与回归的关系：
  - 回归 → 预测连续数值
  - 分类 → 预测离散标签

---

## 2. 问题形式化（Problem Formulation, 15分钟）
- **输入与输出**
  - 输入特征向量：\( x \in \mathbb{R}^d \)
  - 输出类别：\( y \in \{1,2,\dots,K\} \)
- **目标**
  - 学习分类函数 \( f(x) \) 将输入映射到类别
  - 或学习条件概率分布 \( P(y|x) \)
- **决策边界**
  - 分类模型本质：在特征空间中找到“边界”
- **损失函数**
  - 0-1 损失（理论上的标准，但不可优化）
  - surrogate loss（log loss, hinge loss）

---
## 3. 决策树（Decision Trees, 20分钟）https://www.youtube.com/watch?v=zs6yHVtxyv8
- **基本思想**
  - 逐步划分特征空间，构造树状决策过程
- **划分标准**
  - 信息增益（entropy）
- **示例**
  - 学生成绩预测：根据“出勤率”和“作业完成情况”分类
- **Python代码与应用**
- **优缺点**
  - 可解释性强，易于可视化
  - 容易过拟合
- **进阶版本: Random forest** https://www.youtube.com/watch?v=cIbj0WuK41w

---
## 4. 逻辑回归（Logistic Regression, 20分钟） https://www.youtube.com/watch?v=3bvM3NyMiE0
- **从线性回归到分类**
  - 问题：线性回归输出连续值，不适合做类别预测
  - 解决：Sigmoid 函数将实数映射到概率区间 [0,1]
- **模型形式**
- **损失函数**
  - 对数似然（Log-likelihood）
  - 交叉熵损失（Cross-Entropy Loss）
- **优化**
  - 梯度下降
  - 随机梯度下降
- **多分类扩展**
  - Softmax 回归
- **Python代码与应用**
- **优缺点**

---
## 5. 支持向量机（Support Vector Machines, SVM, 20分钟）https://www.youtube.com/watch?v=_YPScrckx28
- **基本思想**
  - 寻找最大间隔超平面（max-margin hyperplane），希望分类边界距离两类样本尽可能远，提高泛化能力

- **线性可分数据：硬间隔 SVM**
  - 目标函数
  - 详细推导对偶问题（Dual Formulation）
  - 详细讲解支持向量的含义

- **线性不可分数据：软间隔 SVM**
  - 目标函数
  - 展示对偶问题和对应的支持向量

- **核技巧（Kernel Trick）** （非常简要）

- **Python代码与应用**
- **优缺点**
  - 理论基础扎实，几何解释清晰
  - 在小数据集、高维空间表现良好
  - 大规模训练代价高
  - 参数（C、核函数、核参数）敏感


## 6. 总结与对比（5分钟）

- **决策树：可解释性强，容易过拟合**

- **逻辑回归：线性模型，概率输出，可扩展到多分类**

- **支持向量机：最大间隔理论，适合小样本高维数据**

- **三者在分类任务中的优缺点对比**