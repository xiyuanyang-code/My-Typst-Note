# 机器学习课程 — 第二节课大纲（Regression, 90分钟）

## 1. 为什么学习回归（10分钟）
- **重要性**
  - 回归是最经典、最基础的机器学习任务
  - 目标是预测连续值（real-valued outputs）
  - 神经网络回归一切
- **激励案例**
  - 学生成绩预测：根据作业成绩、出勤率 → 预测期末分数
  - 天气预测：根据历史温度、湿度 → 预测未来温度
  - 股票预测：根据历史价格、交易量 → 预测次日股价
- **与其他任务的关系**
  - 分类：预测离散标签 vs 连续值预测
  - 强化学习：价值函数估计本质上是回归问题

---
## 2. 线性代数基础回顾（10分钟）
- **向量 (Vector)**
  - 表示一个样本的特征，如 \( x \in \mathbb{R}^d \)
- **矩阵 (Matrix)**
  - 表示整个数据集，如 \( X \in \mathbb{R}^{n \times d} \)，包含 n 个样本、d 个特征
- **转置、点积与范数**
  - 点积：\( x^T y \)
  - 二范数：\( \|x\|_2 = \sqrt{\sum_i x_i^2} \)
- **矩阵运算在建模中的作用**
  - 预测：\( \hat{y} = Xw \)
  - 损失：\( L = \|y - Xw\|^2 \)


## 3. 回归问题的建模与评价（15分钟）"强调数学、尽量使用matrix/vector的表示形式"
- **基本元素** 
  - 数据：输入特征与输出标签
  - 模型：函数映射输入到输出
  - 损失函数：度量预测与真实的差距
- **常见评价指标**
  - MSE（均方误差）
  - MAE（平均绝对误差）
  - R²（决定系数）
---

## 4. KNN 回归（10分钟）
- **核心思想**
  - 预测时，找到最近的 K 个邻居
  - 预测值 = 邻居输出的平均
- **直观示例**
  - 1-NN 与 2-NN 的对比
  - Python代码实现与应用
- **优缺点**
  - 优点：简单直观，非参数方法
  - 缺点：高维数据表现差，预测开销大
---

## 5. 线性回归（20分钟）"强调数学、尽量使用matrix/vector的表示形式" https://www.youtube.com/watch?v=CtsRRUddV2s
- **核心思想**
  - 假设输入与输出存在线性关系
- **矩阵表示**
  - 将问题写成矩阵形式，方便计算
- **最小二乘法**
  - 目标函数
  - 闭式解 
  - Python代码实现与应用
- **梯度下降** https://www.youtube.com/watch?v=qg4PchTECck&list=PLqwozWPBo-FtNyPKLDPTVDOHwK12QbVsM&index=3
  - 更新公式：数值优化方法，适合大规模数据
  - Python代码实现与应用
- **优缺点**
  - 优点：可解释性强，计算高效
  - 缺点：只能建模线性关系
---

## 6. 正则化与改进（15分钟）"强调数学、尽量使用matrix/vector的表示形式"
- **Ridge 回归**
  - 核心思想：L2 正则化，防止过拟合
  - 目标函数
  - 闭式解
- **Lasso 回归**
  - 核心思想：L1 正则化，可产生稀疏解
  - 目标函数
  - 求解
- **Elastic Net**
  - 核心思想：L1 + L2 结合，适合高维数据
  - 目标函数
  - 求解
---

## 7. 神经网络与回归任务（5分钟）"简略介绍下数学上的联系"
- **回归作为神经网络的基础**
  - 最简单的神经网络（单层感知机）就是线性回归模型：
    \[
    \hat{y} = w^T x + b
    \]
  - 如果去掉非线性激活函数，神经网络退化为线性回归

- **多层感知机 (MLP) 与非线性回归**
  - 引入非线性激活函数（ReLU, Sigmoid, Tanh 等），神经网络能够逼近任意连续函数
  - 神经网络的训练目标依然是最小化均方误差 (MSE) 等回归损失：
    \[
    L(\theta) = \frac{1}{n}\sum_{i=1}^n (y_i - f_\theta(x_i))^2
    \]

- **联系**
  - 线性回归是神经网络的「最简原型」
  - 神经网络可以被视为 **“非线性基函数扩展的回归模型”**
  - 从优化角度看：依旧是 **参数化函数拟合 + 损失最小化**

---
## 8. 总结 & 课堂互动（5分钟）
- **本节回顾**
  - 回归任务的重要性
  - KNN 回归与线性回归
  - 最小二乘与梯度下降
  - 正则化方法
- **互动问题**
  - 如果数据关系不是线性的，该怎么办？
  - 为下一节 **分类（Classification）** 做铺垫