{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e80cb5b7",
   "metadata": {
    "papermill": {
     "duration": 0.004115,
     "end_time": "2025-09-10T03:18:32.619450",
     "exception": false,
     "start_time": "2025-09-10T03:18:32.615335",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMDB数据集的二分类情感分析\n",
    "\n",
    "大纲如下：\n",
    "- [Model](#Model-Part)\n",
    "- [DataLoader](#DataLoader-Part)\n",
    "- [Train_loop & Test_loop](#Train_loop-Part)\n",
    "- [Controller](#Controller-Part)\n",
    "- [Console](#Console-Part)\n",
    "- [Evaluater](#Evaluater-Part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36201b57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T03:18:32.627992Z",
     "iopub.status.busy": "2025-09-10T03:18:32.627676Z",
     "iopub.status.idle": "2025-09-10T03:18:38.761946Z",
     "shell.execute_reply": "2025-09-10T03:18:38.761262Z"
    },
    "papermill": {
     "duration": 6.13976,
     "end_time": "2025-09-10T03:18:38.763398",
     "exception": false,
     "start_time": "2025-09-10T03:18:32.623638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Callable\n",
    "from pathlib import Path\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827cdbdc",
   "metadata": {
    "papermill": {
     "duration": 0.002504,
     "end_time": "2025-09-10T03:18:38.768925",
     "exception": false,
     "start_time": "2025-09-10T03:18:38.766421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## `Model` Part\n",
    "\n",
    "### Tranformer\n",
    "- `TransformerBlock`实现Transformer架构的核心编码器块，包含自注意力机制和前馈神经网络。该模块使用残差连接、层归一化和dropout来提高模型性能和稳定性。\n",
    "\n",
    "   `nn.MultiheadAttention`:对于每一个$\\text{head}_{i}$, 从`Input`出发，经过矩阵乘法得到$Q_{i},K_{i},V_{i}$，并对他们使用注意力机制，得到单个头的输出，再用$W^{O}$整合所有头的输出，得到我们最终的`Output`，其中$W^{Q}_{i}, W^{K}_{i}, W^{V}_{i}, W^{O}$是可训练的网络参数：\n",
    "   $$Q_{i}, K_{i}, V_{i} = W^{Q}_{i} \\times \\text{Input}, \\quad W^{K}_{i} \\times \\text{Input}, \\quad W^{V}_{i} \\times \\text{Input}$$  \n",
    "   $$\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_{1}, \\cdots, \\text{head}_{h})W^{O}$$\n",
    "   $$\\text{head}_{i} = \\text{Attention}(Q_{i}, K_{i}, V_{i})$$\n",
    "   $$\\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^{T}}{\\sqrt{d_{k}}})V$$  \n",
    "   `forward`: 多头注意力的输出经过残差连接和层归一化：\n",
    "   $$X_{\\text{norm}} = \\text{LayerNorm}(\\text{Input} + \\text{MultiHead}(Q, K, V))$$  \n",
    "   然后，通过前馈神经网络（FFN）处理：\n",
    "   $$\\text{FFN}(X) = \\text{ReLU}(X \\cdot W_1 + b_1) \\cdot W_2 + b_2$$  \n",
    "   最后，再次应用残差连接和层归一化：\n",
    "   $$\\text{Output} = \\text{LayerNorm}(X_{\\text{norm}} + \\text{FFN}(X_{\\text{norm}}))$$\n",
    "\n",
    "- `TokenAndPositionEmbedding`:Token和位置嵌入模块, 将输入token索引转换为密集向量表示，并添加位置编码信息。位置编码使模型能够理解序列中token的相对位置。\n",
    "\n",
    "- `Transformer`: 基于Transformer的文本分类模型，此模型实现了为文本分类任务设计的简化版Transformer架构。\n",
    "\n",
    "### LSTM\n",
    "\n",
    "- `nn.LSTM`: 应用一个多层的长短期记忆网络（LSTM）到输入序列中。对于输入序列中的每一个元素，每一层执行以下计算：\n",
    "  $$\n",
    "  \\begin{aligned}\n",
    "   i_t &= \\sigma(W_{\\text{ii}}x_t + b_{\\text{ii}} + W_{\\text{hi}}h_{t-1} + b_{\\text{hi}}) \\\\\n",
    "   f_t &= \\sigma(W_{\\text{if}}x_t + b_{\\text{if}} + W_{\\text{hf}}h_{t-1} + b_{\\text{hf}}) \\\\\n",
    "   g_t &= \\tanh(W_{\\text{ig}}x_t + b_{\\text{ig}} + W_{\\text{hg}}h_{t-1} + b_{\\text{hg}}) \\\\\n",
    "   o_t &= \\sigma(W_{\\text{io}}x_t + b_{\\text{io}} + W_{\\text{ho}}h_{t-1} + b_{\\text{ho}}) \\\\\n",
    "   \\color{orange}{c_t} \\, &\\color{orange}{=f_t \\odot c_{t-1} + i_t \\odot g_t} \\\\\n",
    "   h_t &= o_t \\odot \\tanh(c_t)\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "   其中：\n",
    "   - $h_t$ 表示时间步 $t$ 的隐藏状态；\n",
    "   - $c_t$ 表示时间步 $t$ 的神经元状态（cell state）；\n",
    "   - $x_t$ 是时间步 $t$ 的输入；\n",
    "   - $h_{t-1}$ 是前一个时间步的隐藏状态，或者是初始的隐藏状态（当 $t = 0$ 时）；\n",
    "   - $i_t, f_t, g_t, o_t$ 分别是输入门、遗忘门、候选状态、输出门；\n",
    "   - $\\sigma$ 是 sigmoid 激活函数；\n",
    "   - $\\odot$ 表示 Hadamard（元素乘）运算。\n",
    "\n",
    "- `LSTM`: 由一个`lstm`网络和一个全连接网络`FNN`组成\n",
    "\n",
    "   `forward`: \n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   \\text{output}, \\textcolor{orange}{hidden},\\text{cell} = \\text{lstm}(\\text{input}) \\\\\n",
    "   \\text{probs} = \\text{Softmax}(\\text{FNN}(\\textcolor{orange}{hidden}))\n",
    "   \\end{aligned}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45ba133b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T03:18:38.776034Z",
     "iopub.status.busy": "2025-09-10T03:18:38.775614Z",
     "iopub.status.idle": "2025-09-10T03:18:38.790958Z",
     "shell.execute_reply": "2025-09-10T03:18:38.790316Z"
    },
    "papermill": {
     "duration": 0.020574,
     "end_time": "2025-09-10T03:18:38.792189",
     "exception": false,
     "start_time": "2025-09-10T03:18:38.771615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Block\n",
    "\n",
    "    参数:\n",
    "        embed_dim (int): 输入特征的维度\n",
    "        num_heads (int): 注意力头的数量\n",
    "        ff_dim (int): 前馈网络中的隐藏维度\n",
    "        rate (float, optional): Dropout率(默认为0.1)\n",
    "\n",
    "    输入:\n",
    "        inputs (torch.Tensor): 形状为[batch_size, sequence_length, embed_dim]的输入张量\n",
    "\n",
    "    输出:\n",
    "        torch.Tensor: 与输入形状相同的转换后的特征\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        ff_dim: int,\n",
    "        rate: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim, num_heads=num_heads, batch_first=True\n",
    "        )\n",
    "\n",
    "        self.dense_1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.dense_2 = nn.Linear(ff_dim, embed_dim)\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "\n",
    "        self.dropout_1 = nn.Dropout(rate)\n",
    "        self.dropout_2 = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        attn_output, _ = self.attention(inputs, inputs, inputs)\n",
    "        attn_output = self.dropout_1(attn_output)\n",
    "        out1 = self.layer_norm_1(inputs + attn_output)\n",
    "\n",
    "        ffn_output = self.dense_1(out1)\n",
    "        ffn_output = F.relu(ffn_output)\n",
    "        ffn_output = self.dense_2(ffn_output)\n",
    "        ffn_output = self.dropout_2(ffn_output)\n",
    "\n",
    "        return self.layer_norm_2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    参数:\n",
    "        max_length (int): 支持的最大序列长度\n",
    "        vocab_size (int): 词汇表大小\n",
    "        embed_dim (int): 嵌入向量的维度\n",
    "\n",
    "    输入:\n",
    "        x (torch.Tensor): 形状为[batch_size, sequence_length]的整数数组(Token索引)\n",
    "\n",
    "    输出:\n",
    "        torch.Tensor: 形状为[batch_size, sequence_length, embed_dim]的嵌入向量\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_length: int, vocab_size: int, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        pe = torch.zeros(max_length, embed_dim).cuda()\n",
    "        positions = torch.arange(0, max_length, device=pe.device).unsqueeze(1)\n",
    "        # div_term = (torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))).cuda()\n",
    "        div_term = (1 / (10000**(torch.arange(0, embed_dim, 2).float() / embed_dim))).cuda()\n",
    "        pe[:, 0::2] = torch.sin(positions * div_term)\n",
    "        pe[:, 1::2] = torch.cos(positions * div_term)\n",
    "\n",
    "        self.register_buffer(\"positional_encoding\", pe)\n",
    "        \n",
    "        # self.pos_emb = nn.Embedding(max_length, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        maxlen = x.size(-1)\n",
    "        # positions = self.pos_emb(positions)\n",
    "        \n",
    "        x = self.token_emb(x)\n",
    "        pos_enc = self.positional_encoding[:maxlen,:].unsqueeze(0)\n",
    "        return x + pos_enc\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "        embed_dim (int): token嵌入的维度大小\n",
    "        num_heads (int): 注意力头的数量\n",
    "        ff_dim (int): 前馈网络中的隐藏层大小\n",
    "        num_block(int): Transformer Block的数量\n",
    "        maxlen (int): 最大输入序列长度\n",
    "        vocab_size (int): 词汇表大小\n",
    "\n",
    "    输入:\n",
    "        x (torch.Tensor): 形状为[batch_size, sequence_length]的整数数组(Token索引)\n",
    "\n",
    "    输出:\n",
    "        torch.Tensor: 形状为[batch_size, 2]的浮点数组(两个类别的概率分布)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        ff_dim: int,\n",
    "        num_blocks:int, \n",
    "        maxlen: int,\n",
    "        vocab_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dense1 = nn.Linear(embed_dim, 20)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.dense2 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.dense1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.dense2(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, \n",
    "                vocab_size : int, \n",
    "                embedding_dim : int, \n",
    "                hidden_dim : int, \n",
    "                n_layers:int = 1,\n",
    "                dropout:float = 0.2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers = n_layers, \n",
    "                            batch_first=True, \n",
    "                            dropout=dropout if n_layers > 1 else 0\n",
    "                            )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 2) # 二分类问题\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        _, (hidden_t, _) = self.lstm(embedded)\n",
    "        logits = self.fc(self.dropout(hidden_t[-1, :, :]))\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9956c264",
   "metadata": {
    "papermill": {
     "duration": 0.002568,
     "end_time": "2025-09-10T03:18:38.797719",
     "exception": false,
     "start_time": "2025-09-10T03:18:38.795151",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## `DataLoader` Part\n",
    "\n",
    "- `IMDBDataset`：我们实现了`Pytorch`抽象类`Dataset`的一个子类，用于我们文本分类的任务。（所有`Dataset`子类必须要实现`__getitem__`方法，可选择实现`__len__`方法用于返回数据集的大小）\n",
    "\n",
    "`prepare_data_loader`: 准备训练和测试的数据加载器的函数。此函数加载数据集，将其分为训练集和测试集，并创建相应的数据加载器。数据加载过程使用`PyTorch`的`DataLoader`进行并行处理，以提高数据加载效率。工作流程：  \n",
    "- 从指定路径加载数据集\n",
    "- 根据给定比例将数据集分为训练集和测试集\n",
    "- 为训练集和测试集创建`Dataset`对象\n",
    "- 配置数据加载器，设置批量大小、工作进程数等\n",
    "\n",
    "返回一个包含训练和测试数据集的字典（{\"train\": `train_loader`, \"test\": `test_loader`}）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf987bae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T03:18:38.804178Z",
     "iopub.status.busy": "2025-09-10T03:18:38.803604Z",
     "iopub.status.idle": "2025-09-10T03:18:38.810658Z",
     "shell.execute_reply": "2025-09-10T03:18:38.809920Z"
    },
    "papermill": {
     "duration": 0.011529,
     "end_time": "2025-09-10T03:18:38.811860",
     "exception": false,
     "start_time": "2025-09-10T03:18:38.800331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"encoded_indices\": torch.tensor(self._x[idx], dtype=torch.long),\n",
    "            \"label\": torch.tensor(self._y[idx], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._x)\n",
    "\n",
    "\n",
    "def prepare_data_loader(\n",
    "    path: str,\n",
    "    ratio: float,\n",
    "    batch_size: int,\n",
    "    num_workers: int = 4,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    参数:\n",
    "        path (str): .npz格式的数据集文件路径\n",
    "        ratio (float, optional): 用于训练的数据比例(默认为0.8)\n",
    "        seed (int, optional): 用于随机打乱的种子(默认为12)\n",
    "        batch_size (int, optional): 训练批量大小(默认为128)\n",
    "        num_workers (int, optional): 数据加载的工作进程数(默认为4)\n",
    "\n",
    "    返回:\n",
    "        dict: 包含训练和测试数据加载器的字典\n",
    "    \"\"\"\n",
    "    train_data = np.load(path)\n",
    "\n",
    "    x_data = train_data[\"x_train\"]\n",
    "    y_data = train_data[\"y_train\"]\n",
    "\n",
    "    num_samples = len(x_data)\n",
    "    split_idx = int(num_samples * ratio)\n",
    "    x_train = x_data[:split_idx]\n",
    "    y_train = y_data[:split_idx]\n",
    "    x_test = x_data[split_idx:]\n",
    "    y_test = y_data[split_idx:]\n",
    "\n",
    "    train_batch_size = batch_size\n",
    "    test_batch_size = train_batch_size\n",
    "\n",
    "    # 创建PyTorch数据集\n",
    "    train_dataset = IMDBDataset(x_train, y_train)\n",
    "    test_dataset = IMDBDataset(x_test, y_test)\n",
    "\n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return {\"train\": train_loader, \"test\": test_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3350e9",
   "metadata": {
    "papermill": {
     "duration": 0.002597,
     "end_time": "2025-09-10T03:18:38.817248",
     "exception": false,
     "start_time": "2025-09-10T03:18:38.814651",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## `Train_loop` Part\n",
    "这一部分是实现神经网络在一个训练周期中的两个关键步骤：\n",
    "- 在训练集上面计算损失，并依据损失更新参数\n",
    "- 在测试集上面计算度量(Metric)，跟踪当前参数下的泛化性能\n",
    "\n",
    "我们实现了`train_step`和`eval_step`两个函数分别执行单步(step)的训练步骤和测试步骤，`train_per_epoch`和`test_per_epoch`两个函数分别执行一个回合(epoch)的训练步骤和测试步骤。对于深度学习而言，一个`step`包含对一批(`batch`)数据的处理，一个`epoch`包含多个`step`，当把数据集的数据都训练过一遍，我们称为一个`epoch`，也就是说当数据集能被`batch`整除的时候，有:\n",
    "$$ \\text{batch\\_sizes} * \\text{num\\_steps} = \\text{total\\_data} $$\n",
    "\n",
    "`train_per_epoch` : 此函数处理一个完整`epoch`的训练过程:\n",
    "- 设置模型为训练模式\n",
    "- 遍历训练数据加载器中的所有批次\n",
    "- 对每个批次执行前向和后向传播并使用优化器更新模型参数\n",
    "- 使用进度条跟踪和显示训练指标\n",
    "\n",
    "`test_per_epoch` : 此函数处理一个完整`epoch`训练后的评估过程:\n",
    "- 设置模型为评估模式\n",
    "- 遍历测试数据加载器中的所有批次\n",
    "- 在不更新梯度的情况下计算前向传播\n",
    "- 计算评估指标（损失、准确率等），记录并显示评估结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b2c05b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T03:18:38.823661Z",
     "iopub.status.busy": "2025-09-10T03:18:38.823363Z",
     "iopub.status.idle": "2025-09-10T03:18:38.833007Z",
     "shell.execute_reply": "2025-09-10T03:18:38.832325Z"
    },
    "papermill": {
     "duration": 0.014304,
     "end_time": "2025-09-10T03:18:38.834208",
     "exception": false,
     "start_time": "2025-09-10T03:18:38.819904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    model: nn.Module, \n",
    "    loss_fn: Callable, \n",
    "    optimizer: optim.Optimizer, \n",
    "    batch: dict, \n",
    "    device: torch.device\n",
    "):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "        model (nn.Module): 要训练的模型\n",
    "        loss_fn (Callable): 损失函数\n",
    "        optimizer (optim.Optimizer): 用于更新模型参数的优化器\n",
    "        batch (dict): 包含训练数据的批次\n",
    "        device (torch.device): 用于计算的设备(gpu or cpu)\n",
    "    返回:\n",
    "        float: 当前批次的损失值\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    batch_tokens = batch[\"encoded_indices\"].to(device)\n",
    "    labels = batch[\"label\"].to(device)\n",
    "\n",
    "    loss = loss_fn(model(batch_tokens), labels)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def eval_step(model: nn.Module, metric_fn: Callable, batch: dict, device: torch.device):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "        model (nn.Module): 用于测试的模型\n",
    "        metric_fn (Callable): 测试集上的度量函数\n",
    "        batch (dict): 测试数据的批次\n",
    "        device (torch.device): 用于计算的设备(gpu or cpu)\n",
    "\n",
    "    返回:\n",
    "        tuple: (loss, logits, labels) - 损失值、预测结果和真实标签\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_tokens = batch[\"encoded_indices\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        logits = model(batch_tokens)\n",
    "        metric = metric_fn(logits, labels)\n",
    "\n",
    "        return metric.item(), logits, labels\n",
    "\n",
    "\n",
    "def train_per_epoch(\n",
    "    model: nn.Module,\n",
    "    loss_fn: Callable,\n",
    "    optimizer: optim.Optimizer,\n",
    "    batch_size: int,\n",
    "    train_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "        model (nn.Module): 训练模型\n",
    "        optimizer (optim.Optimizer): 用于更新模型参数的优化器\n",
    "        train_loader (DataLoader): 包含训练数据的DataLoader\n",
    "        device (torch.device): 用于计算的设备\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    length = len(train_loader.dataset)\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        loss = train_step(model, loss_fn, optimizer, batch, device)\n",
    "        if batch_idx % 20 == 0:\n",
    "            current = batch_idx * batch_size + len(batch[\"encoded_indices\"])\n",
    "            print(f\" Loss: {loss:>6.4f}, {current:>5d}/{length:>5d}\")\n",
    "\n",
    "\n",
    "def test_per_epoch(\n",
    "    model: nn.Module,\n",
    "    metric_fn: Callable,\n",
    "    test_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "        model (nn.Module): 测试模型\n",
    "        test_loader (DataLoader): 包含测试数据的DataLoader\n",
    "        device (torch.device): 用于计算的设备\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    num_batches = len(test_loader)\n",
    "    num_data = len(test_loader.dataset)\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            loss, logits, labels = eval_step(model, metric_fn, batch, device)\n",
    "            total_loss += loss\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    accuracy = correct / num_data\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {avg_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730cb354",
   "metadata": {
    "papermill": {
     "duration": 0.002689,
     "end_time": "2025-09-10T03:18:38.839749",
     "exception": false,
     "start_time": "2025-09-10T03:18:38.837060",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## `Controller` Part\n",
    "这一部分是我们这份代码的控制中枢，我们实例化上文已经写好的类，调用上文已经写好的各个函数，组合起来实现我们的IMDB数据集的情感分析任务。在这一部分，我们要指定需要设定的绝大部分超参数，开展训练并得到我们训练好的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71841e57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T03:18:38.846312Z",
     "iopub.status.busy": "2025-09-10T03:18:38.845805Z",
     "iopub.status.idle": "2025-09-10T03:18:38.852804Z",
     "shell.execute_reply": "2025-09-10T03:18:38.852132Z"
    },
    "papermill": {
     "duration": 0.01149,
     "end_time": "2025-09-10T03:18:38.853999",
     "exception": false,
     "start_time": "2025-09-10T03:18:38.842509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def controller(seed: int,\n",
    "               model_type: str,\n",
    "               embed_dim: int, \n",
    "               num_heads: int, \n",
    "               ff_dim: int,\n",
    "               num_blocks:int,\n",
    "               lstm_embed: int,\n",
    "               hidden_dim: int,\n",
    "               ratio: float,\n",
    "               batch_size: int, \n",
    "               epochs: int, \n",
    "               learning_rate: float):\n",
    "    torch.manual_seed(seed)\n",
    "    data_path = Path(\"/kaggle/input/lstm-transformers-2/processed_imdb_train_data.npz\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if model_type == \"Transformer\":\n",
    "        model = Transformer(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            ff_dim=ff_dim,\n",
    "            num_blocks=num_blocks,\n",
    "            maxlen=200,\n",
    "            vocab_size=20000,\n",
    "        ).to(device)\n",
    "    elif model_type == \"LSTM\":\n",
    "        model = LSTM(\n",
    "            vocab_size=20000,\n",
    "            embedding_dim=lstm_embed,\n",
    "            hidden_dim=hidden_dim,\n",
    "        ).to(device)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    metric_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    loader_dict = prepare_data_loader(data_path, ratio, batch_size)\n",
    "    train_loader = loader_dict[\"train\"]\n",
    "    test_loader = loader_dict[\"test\"]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1} \\n--------------------------------\")\n",
    "        train_per_epoch(model, loss_fn, optimizer, batch_size, train_loader, device)\n",
    "        # scheduler.step() # 每个epoch结束时更新学习率\n",
    "        test_per_epoch(model, metric_fn, test_loader, device)\n",
    "    print(\"Done!\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac69fc4c",
   "metadata": {
    "papermill": {
     "duration": 0.002565,
     "end_time": "2025-09-10T03:18:38.859160",
     "exception": false,
     "start_time": "2025-09-10T03:18:38.856595",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## `Console` Part\n",
    "这个板块是我们的总控制台，在这里我们设定所有的超参数，调用我们前面写好的所有类和函数，训练模型并返回我们的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0c949dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T03:18:38.865908Z",
     "iopub.status.busy": "2025-09-10T03:18:38.865336Z",
     "iopub.status.idle": "2025-09-10T03:19:46.008938Z",
     "shell.execute_reply": "2025-09-10T03:19:46.007605Z"
    },
    "papermill": {
     "duration": 67.148693,
     "end_time": "2025-09-10T03:19:46.010485",
     "exception": false,
     "start_time": "2025-09-10T03:18:38.861792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      "--------------------------------\n",
      " Loss: 0.6960,   128/20000\n",
      " Loss: 0.6950,  2688/20000\n",
      " Loss: 0.6900,  5248/20000\n",
      " Loss: 0.6948,  7808/20000\n",
      " Loss: 0.6952, 10368/20000\n",
      " Loss: 0.6887, 12928/20000\n",
      " Loss: 0.6889, 15488/20000\n",
      " Loss: 0.6905, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 0.688578 \n",
      "\n",
      "Epoch 2 \n",
      "--------------------------------\n",
      " Loss: 0.6846,   128/20000\n",
      " Loss: 0.6821,  2688/20000\n",
      " Loss: 0.6824,  5248/20000\n",
      " Loss: 0.6697,  7808/20000\n",
      " Loss: 0.6732, 10368/20000\n",
      " Loss: 0.6798, 12928/20000\n",
      " Loss: 0.6390, 15488/20000\n",
      " Loss: 0.6440, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.659351 \n",
      "\n",
      "Epoch 3 \n",
      "--------------------------------\n",
      " Loss: 0.6723,   128/20000\n",
      " Loss: 0.6286,  2688/20000\n",
      " Loss: 0.6268,  5248/20000\n",
      " Loss: 0.6080,  7808/20000\n",
      " Loss: 0.6483, 10368/20000\n",
      " Loss: 0.6585, 12928/20000\n",
      " Loss: 0.6586, 15488/20000\n",
      " Loss: 0.5832, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.619531 \n",
      "\n",
      "Epoch 4 \n",
      "--------------------------------\n",
      " Loss: 0.6138,   128/20000\n",
      " Loss: 0.5720,  2688/20000\n",
      " Loss: 0.5831,  5248/20000\n",
      " Loss: 0.6191,  7808/20000\n",
      " Loss: 0.6097, 10368/20000\n",
      " Loss: 0.5659, 12928/20000\n",
      " Loss: 0.6324, 15488/20000\n",
      " Loss: 0.5545, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.594985 \n",
      "\n",
      "Epoch 5 \n",
      "--------------------------------\n",
      " Loss: 0.5918,   128/20000\n",
      " Loss: 0.5604,  2688/20000\n",
      " Loss: 0.5943,  5248/20000\n",
      " Loss: 0.5662,  7808/20000\n",
      " Loss: 0.6030, 10368/20000\n",
      " Loss: 0.6073, 12928/20000\n",
      " Loss: 0.5531, 15488/20000\n",
      " Loss: 0.5758, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.611960 \n",
      "\n",
      "Epoch 6 \n",
      "--------------------------------\n",
      " Loss: 0.5408,   128/20000\n",
      " Loss: 0.5240,  2688/20000\n",
      " Loss: 0.5891,  5248/20000\n",
      " Loss: 0.6065,  7808/20000\n",
      " Loss: 0.5325, 10368/20000\n",
      " Loss: 0.5346, 12928/20000\n",
      " Loss: 0.5292, 15488/20000\n",
      " Loss: 0.4968, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.595715 \n",
      "\n",
      "Epoch 7 \n",
      "--------------------------------\n",
      " Loss: 0.5521,   128/20000\n",
      " Loss: 0.5509,  2688/20000\n",
      " Loss: 0.5855,  5248/20000\n",
      " Loss: 0.5646,  7808/20000\n",
      " Loss: 0.5131, 10368/20000\n",
      " Loss: 0.4959, 12928/20000\n",
      " Loss: 0.5566, 15488/20000\n",
      " Loss: 0.5442, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.540295 \n",
      "\n",
      "Epoch 8 \n",
      "--------------------------------\n",
      " Loss: 0.5256,   128/20000\n",
      " Loss: 0.5644,  2688/20000\n",
      " Loss: 0.5231,  5248/20000\n",
      " Loss: 0.5474,  7808/20000\n",
      " Loss: 0.5554, 10368/20000\n",
      " Loss: 0.5706, 12928/20000\n",
      " Loss: 0.5355, 15488/20000\n",
      " Loss: 0.4908, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.562547 \n",
      "\n",
      "Epoch 9 \n",
      "--------------------------------\n",
      " Loss: 0.5019,   128/20000\n",
      " Loss: 0.4909,  2688/20000\n",
      " Loss: 0.5431,  5248/20000\n",
      " Loss: 0.4966,  7808/20000\n",
      " Loss: 0.4785, 10368/20000\n",
      " Loss: 0.5277, 12928/20000\n",
      " Loss: 0.5200, 15488/20000\n",
      " Loss: 0.4708, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.525712 \n",
      "\n",
      "Epoch 10 \n",
      "--------------------------------\n",
      " Loss: 0.4866,   128/20000\n",
      " Loss: 0.4838,  2688/20000\n",
      " Loss: 0.5324,  5248/20000\n",
      " Loss: 0.4755,  7808/20000\n",
      " Loss: 0.4709, 10368/20000\n",
      " Loss: 0.4823, 12928/20000\n",
      " Loss: 0.5795, 15488/20000\n",
      " Loss: 0.4896, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.553178 \n",
      "\n",
      "Epoch 11 \n",
      "--------------------------------\n",
      " Loss: 0.5310,   128/20000\n",
      " Loss: 0.5283,  2688/20000\n",
      " Loss: 0.5041,  5248/20000\n",
      " Loss: 0.4912,  7808/20000\n",
      " Loss: 0.4582, 10368/20000\n",
      " Loss: 0.4687, 12928/20000\n",
      " Loss: 0.4473, 15488/20000\n",
      " Loss: 0.5017, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.556370 \n",
      "\n",
      "Epoch 12 \n",
      "--------------------------------\n",
      " Loss: 0.4417,   128/20000\n",
      " Loss: 0.4756,  2688/20000\n",
      " Loss: 0.4596,  5248/20000\n",
      " Loss: 0.4719,  7808/20000\n",
      " Loss: 0.4813, 10368/20000\n",
      " Loss: 0.4977, 12928/20000\n",
      " Loss: 0.4810, 15488/20000\n",
      " Loss: 0.4614, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.561581 \n",
      "\n",
      "Epoch 13 \n",
      "--------------------------------\n",
      " Loss: 0.4468,   128/20000\n",
      " Loss: 0.4819,  2688/20000\n",
      " Loss: 0.4685,  5248/20000\n",
      " Loss: 0.4963,  7808/20000\n",
      " Loss: 0.5024, 10368/20000\n",
      " Loss: 0.4915, 12928/20000\n",
      " Loss: 0.4761, 15488/20000\n",
      " Loss: 0.4674, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.508793 \n",
      "\n",
      "Epoch 14 \n",
      "--------------------------------\n",
      " Loss: 0.5165,   128/20000\n",
      " Loss: 0.5454,  2688/20000\n",
      " Loss: 0.4971,  5248/20000\n",
      " Loss: 0.4612,  7808/20000\n",
      " Loss: 0.4493, 10368/20000\n",
      " Loss: 0.4765, 12928/20000\n",
      " Loss: 0.4967, 15488/20000\n",
      " Loss: 0.4539, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.546566 \n",
      "\n",
      "Epoch 15 \n",
      "--------------------------------\n",
      " Loss: 0.4946,   128/20000\n",
      " Loss: 0.4597,  2688/20000\n",
      " Loss: 0.4774,  5248/20000\n",
      " Loss: 0.4881,  7808/20000\n",
      " Loss: 0.4851, 10368/20000\n",
      " Loss: 0.4426, 12928/20000\n",
      " Loss: 0.4846, 15488/20000\n",
      " Loss: 0.5038, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.521724 \n",
      "\n",
      "Epoch 16 \n",
      "--------------------------------\n",
      " Loss: 0.4355,   128/20000\n",
      " Loss: 0.4913,  2688/20000\n",
      " Loss: 0.4626,  5248/20000\n",
      " Loss: 0.4908,  7808/20000\n",
      " Loss: 0.4272, 10368/20000\n",
      " Loss: 0.4434, 12928/20000\n",
      " Loss: 0.4494, 15488/20000\n",
      " Loss: 0.4377, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.550783 \n",
      "\n",
      "Epoch 17 \n",
      "--------------------------------\n",
      " Loss: 0.4311,   128/20000\n",
      " Loss: 0.4695,  2688/20000\n",
      " Loss: 0.5278,  5248/20000\n",
      " Loss: 0.5044,  7808/20000\n",
      " Loss: 0.5057, 10368/20000\n",
      " Loss: 0.4926, 12928/20000\n",
      " Loss: 0.3971, 15488/20000\n",
      " Loss: 0.4414, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.515960 \n",
      "\n",
      "Epoch 18 \n",
      "--------------------------------\n",
      " Loss: 0.4406,   128/20000\n",
      " Loss: 0.4622,  2688/20000\n",
      " Loss: 0.4699,  5248/20000\n",
      " Loss: 0.4865,  7808/20000\n",
      " Loss: 0.4734, 10368/20000\n",
      " Loss: 0.4283, 12928/20000\n",
      " Loss: 0.4531, 15488/20000\n",
      " Loss: 0.4696, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.509616 \n",
      "\n",
      "Epoch 19 \n",
      "--------------------------------\n",
      " Loss: 0.4817,   128/20000\n",
      " Loss: 0.4998,  2688/20000\n",
      " Loss: 0.4257,  5248/20000\n",
      " Loss: 0.4252,  7808/20000\n",
      " Loss: 0.4645, 10368/20000\n",
      " Loss: 0.4600, 12928/20000\n",
      " Loss: 0.4704, 15488/20000\n",
      " Loss: 0.4567, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.534394 \n",
      "\n",
      "Epoch 20 \n",
      "--------------------------------\n",
      " Loss: 0.4200,   128/20000\n",
      " Loss: 0.4461,  2688/20000\n",
      " Loss: 0.4052,  5248/20000\n",
      " Loss: 0.4484,  7808/20000\n",
      " Loss: 0.4526, 10368/20000\n",
      " Loss: 0.4700, 12928/20000\n",
      " Loss: 0.4178, 15488/20000\n",
      " Loss: 0.4357, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.513106 \n",
      "\n",
      "Epoch 21 \n",
      "--------------------------------\n",
      " Loss: 0.4722,   128/20000\n",
      " Loss: 0.4812,  2688/20000\n",
      " Loss: 0.4526,  5248/20000\n",
      " Loss: 0.4357,  7808/20000\n",
      " Loss: 0.4643, 10368/20000\n",
      " Loss: 0.4692, 12928/20000\n",
      " Loss: 0.4839, 15488/20000\n",
      " Loss: 0.4366, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.521747 \n",
      "\n",
      "Epoch 22 \n",
      "--------------------------------\n",
      " Loss: 0.4544,   128/20000\n",
      " Loss: 0.4371,  2688/20000\n",
      " Loss: 0.4603,  5248/20000\n",
      " Loss: 0.4321,  7808/20000\n",
      " Loss: 0.4480, 10368/20000\n",
      " Loss: 0.4205, 12928/20000\n",
      " Loss: 0.4691, 15488/20000\n",
      " Loss: 0.4451, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.510133 \n",
      "\n",
      "Epoch 23 \n",
      "--------------------------------\n",
      " Loss: 0.4258,   128/20000\n",
      " Loss: 0.4408,  2688/20000\n",
      " Loss: 0.5300,  5248/20000\n",
      " Loss: 0.4136,  7808/20000\n",
      " Loss: 0.4218, 10368/20000\n",
      " Loss: 0.4309, 12928/20000\n",
      " Loss: 0.4366, 15488/20000\n",
      " Loss: 0.4762, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.494414 \n",
      "\n",
      "Epoch 24 \n",
      "--------------------------------\n",
      " Loss: 0.4413,   128/20000\n",
      " Loss: 0.4308,  2688/20000\n",
      " Loss: 0.4364,  5248/20000\n",
      " Loss: 0.4660,  7808/20000\n",
      " Loss: 0.4505, 10368/20000\n",
      " Loss: 0.4615, 12928/20000\n",
      " Loss: 0.4698, 15488/20000\n",
      " Loss: 0.4383, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.503329 \n",
      "\n",
      "Epoch 25 \n",
      "--------------------------------\n",
      " Loss: 0.4071,   128/20000\n",
      " Loss: 0.3907,  2688/20000\n",
      " Loss: 0.4359,  5248/20000\n",
      " Loss: 0.4388,  7808/20000\n",
      " Loss: 0.4634, 10368/20000\n",
      " Loss: 0.3989, 12928/20000\n",
      " Loss: 0.3877, 15488/20000\n",
      " Loss: 0.4389, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.506213 \n",
      "\n",
      "Epoch 26 \n",
      "--------------------------------\n",
      " Loss: 0.4260,   128/20000\n",
      " Loss: 0.4835,  2688/20000\n",
      " Loss: 0.4369,  5248/20000\n",
      " Loss: 0.4257,  7808/20000\n",
      " Loss: 0.4335, 10368/20000\n",
      " Loss: 0.4618, 12928/20000\n",
      " Loss: 0.4244, 15488/20000\n",
      " Loss: 0.5113, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.488117 \n",
      "\n",
      "Epoch 27 \n",
      "--------------------------------\n",
      " Loss: 0.4462,   128/20000\n",
      " Loss: 0.4266,  2688/20000\n",
      " Loss: 0.4329,  5248/20000\n",
      " Loss: 0.4140,  7808/20000\n",
      " Loss: 0.3759, 10368/20000\n",
      " Loss: 0.4264, 12928/20000\n",
      " Loss: 0.4418, 15488/20000\n",
      " Loss: 0.4206, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.524580 \n",
      "\n",
      "Epoch 28 \n",
      "--------------------------------\n",
      " Loss: 0.4753,   128/20000\n",
      " Loss: 0.4663,  2688/20000\n",
      " Loss: 0.3923,  5248/20000\n",
      " Loss: 0.4135,  7808/20000\n",
      " Loss: 0.3797, 10368/20000\n",
      " Loss: 0.4506, 12928/20000\n",
      " Loss: 0.4022, 15488/20000\n",
      " Loss: 0.4115, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.479172 \n",
      "\n",
      "Epoch 29 \n",
      "--------------------------------\n",
      " Loss: 0.4568,   128/20000\n",
      " Loss: 0.3905,  2688/20000\n",
      " Loss: 0.4134,  5248/20000\n",
      " Loss: 0.4477,  7808/20000\n",
      " Loss: 0.4254, 10368/20000\n",
      " Loss: 0.4124, 12928/20000\n",
      " Loss: 0.3855, 15488/20000\n",
      " Loss: 0.3845, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.524272 \n",
      "\n",
      "Epoch 30 \n",
      "--------------------------------\n",
      " Loss: 0.4191,   128/20000\n",
      " Loss: 0.4428,  2688/20000\n",
      " Loss: 0.3884,  5248/20000\n",
      " Loss: 0.4528,  7808/20000\n",
      " Loss: 0.4143, 10368/20000\n",
      " Loss: 0.3920, 12928/20000\n",
      " Loss: 0.4601, 15488/20000\n",
      " Loss: 0.4301, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.499718 \n",
      "\n",
      "Epoch 31 \n",
      "--------------------------------\n",
      " Loss: 0.4051,   128/20000\n",
      " Loss: 0.3902,  2688/20000\n",
      " Loss: 0.4357,  5248/20000\n",
      " Loss: 0.4545,  7808/20000\n",
      " Loss: 0.4362, 10368/20000\n",
      " Loss: 0.4286, 12928/20000\n",
      " Loss: 0.4104, 15488/20000\n",
      " Loss: 0.4371, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.497044 \n",
      "\n",
      "Epoch 32 \n",
      "--------------------------------\n",
      " Loss: 0.4067,   128/20000\n",
      " Loss: 0.3918,  2688/20000\n",
      " Loss: 0.4200,  5248/20000\n",
      " Loss: 0.4005,  7808/20000\n",
      " Loss: 0.4198, 10368/20000\n",
      " Loss: 0.3962, 12928/20000\n",
      " Loss: 0.4475, 15488/20000\n",
      " Loss: 0.4047, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.509137 \n",
      "\n",
      "Epoch 33 \n",
      "--------------------------------\n",
      " Loss: 0.3640,   128/20000\n",
      " Loss: 0.4046,  2688/20000\n",
      " Loss: 0.3992,  5248/20000\n",
      " Loss: 0.4197,  7808/20000\n",
      " Loss: 0.3797, 10368/20000\n",
      " Loss: 0.4041, 12928/20000\n",
      " Loss: 0.4171, 15488/20000\n",
      " Loss: 0.4586, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.484635 \n",
      "\n",
      "Epoch 34 \n",
      "--------------------------------\n",
      " Loss: 0.4090,   128/20000\n",
      " Loss: 0.4087,  2688/20000\n",
      " Loss: 0.3811,  5248/20000\n",
      " Loss: 0.4207,  7808/20000\n",
      " Loss: 0.3980, 10368/20000\n",
      " Loss: 0.4103, 12928/20000\n",
      " Loss: 0.3893, 15488/20000\n",
      " Loss: 0.3986, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.477288 \n",
      "\n",
      "Epoch 35 \n",
      "--------------------------------\n",
      " Loss: 0.4461,   128/20000\n",
      " Loss: 0.4576,  2688/20000\n",
      " Loss: 0.4537,  5248/20000\n",
      " Loss: 0.4644,  7808/20000\n",
      " Loss: 0.4247, 10368/20000\n",
      " Loss: 0.4237, 12928/20000\n",
      " Loss: 0.4055, 15488/20000\n",
      " Loss: 0.4134, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.475122 \n",
      "\n",
      "Epoch 36 \n",
      "--------------------------------\n",
      " Loss: 0.4091,   128/20000\n",
      " Loss: 0.3995,  2688/20000\n",
      " Loss: 0.4361,  5248/20000\n",
      " Loss: 0.3668,  7808/20000\n",
      " Loss: 0.4187, 10368/20000\n",
      " Loss: 0.3799, 12928/20000\n",
      " Loss: 0.4173, 15488/20000\n",
      " Loss: 0.3822, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.482896 \n",
      "\n",
      "Epoch 37 \n",
      "--------------------------------\n",
      " Loss: 0.4005,   128/20000\n",
      " Loss: 0.3676,  2688/20000\n",
      " Loss: 0.4202,  5248/20000\n",
      " Loss: 0.4159,  7808/20000\n",
      " Loss: 0.4146, 10368/20000\n",
      " Loss: 0.3957, 12928/20000\n",
      " Loss: 0.4364, 15488/20000\n",
      " Loss: 0.4141, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.464764 \n",
      "\n",
      "Epoch 38 \n",
      "--------------------------------\n",
      " Loss: 0.4048,   128/20000\n",
      " Loss: 0.4217,  2688/20000\n",
      " Loss: 0.4152,  5248/20000\n",
      " Loss: 0.4179,  7808/20000\n",
      " Loss: 0.4601, 10368/20000\n",
      " Loss: 0.4004, 12928/20000\n",
      " Loss: 0.3842, 15488/20000\n",
      " Loss: 0.3858, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.479370 \n",
      "\n",
      "Epoch 39 \n",
      "--------------------------------\n",
      " Loss: 0.4254,   128/20000\n",
      " Loss: 0.3971,  2688/20000\n",
      " Loss: 0.4189,  5248/20000\n",
      " Loss: 0.3797,  7808/20000\n",
      " Loss: 0.3512, 10368/20000\n",
      " Loss: 0.4205, 12928/20000\n",
      " Loss: 0.4039, 15488/20000\n",
      " Loss: 0.3909, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.482455 \n",
      "\n",
      "Epoch 40 \n",
      "--------------------------------\n",
      " Loss: 0.3924,   128/20000\n",
      " Loss: 0.3885,  2688/20000\n",
      " Loss: 0.4183,  5248/20000\n",
      " Loss: 0.3912,  7808/20000\n",
      " Loss: 0.3638, 10368/20000\n",
      " Loss: 0.3748, 12928/20000\n",
      " Loss: 0.3962, 15488/20000\n",
      " Loss: 0.3989, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.481458 \n",
      "\n",
      "Epoch 41 \n",
      "--------------------------------\n",
      " Loss: 0.4088,   128/20000\n",
      " Loss: 0.3777,  2688/20000\n",
      " Loss: 0.3864,  5248/20000\n",
      " Loss: 0.4006,  7808/20000\n",
      " Loss: 0.3956, 10368/20000\n",
      " Loss: 0.4176, 12928/20000\n",
      " Loss: 0.4033, 15488/20000\n",
      " Loss: 0.4234, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.471309 \n",
      "\n",
      "Epoch 42 \n",
      "--------------------------------\n",
      " Loss: 0.3880,   128/20000\n",
      " Loss: 0.3924,  2688/20000\n",
      " Loss: 0.3902,  5248/20000\n",
      " Loss: 0.4061,  7808/20000\n",
      " Loss: 0.3628, 10368/20000\n",
      " Loss: 0.3815, 12928/20000\n",
      " Loss: 0.3805, 15488/20000\n",
      " Loss: 0.4407, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.468136 \n",
      "\n",
      "Epoch 43 \n",
      "--------------------------------\n",
      " Loss: 0.3931,   128/20000\n",
      " Loss: 0.3999,  2688/20000\n",
      " Loss: 0.4037,  5248/20000\n",
      " Loss: 0.4075,  7808/20000\n",
      " Loss: 0.4065, 10368/20000\n",
      " Loss: 0.3880, 12928/20000\n",
      " Loss: 0.4046, 15488/20000\n",
      " Loss: 0.3754, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.485303 \n",
      "\n",
      "Epoch 44 \n",
      "--------------------------------\n",
      " Loss: 0.3951,   128/20000\n",
      " Loss: 0.3802,  2688/20000\n",
      " Loss: 0.4114,  5248/20000\n",
      " Loss: 0.3863,  7808/20000\n",
      " Loss: 0.4237, 10368/20000\n",
      " Loss: 0.3748, 12928/20000\n",
      " Loss: 0.4297, 15488/20000\n",
      " Loss: 0.3971, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.477924 \n",
      "\n",
      "Epoch 45 \n",
      "--------------------------------\n",
      " Loss: 0.3897,   128/20000\n",
      " Loss: 0.4097,  2688/20000\n",
      " Loss: 0.4131,  5248/20000\n",
      " Loss: 0.3885,  7808/20000\n",
      " Loss: 0.3732, 10368/20000\n",
      " Loss: 0.3817, 12928/20000\n",
      " Loss: 0.3898, 15488/20000\n",
      " Loss: 0.3673, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.473205 \n",
      "\n",
      "Epoch 46 \n",
      "--------------------------------\n",
      " Loss: 0.4093,   128/20000\n",
      " Loss: 0.4011,  2688/20000\n",
      " Loss: 0.3891,  5248/20000\n",
      " Loss: 0.3683,  7808/20000\n",
      " Loss: 0.4134, 10368/20000\n",
      " Loss: 0.4027, 12928/20000\n",
      " Loss: 0.3694, 15488/20000\n",
      " Loss: 0.3734, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.466142 \n",
      "\n",
      "Epoch 47 \n",
      "--------------------------------\n",
      " Loss: 0.4015,   128/20000\n",
      " Loss: 0.3890,  2688/20000\n",
      " Loss: 0.4028,  5248/20000\n",
      " Loss: 0.4028,  7808/20000\n",
      " Loss: 0.3613, 10368/20000\n",
      " Loss: 0.3730, 12928/20000\n",
      " Loss: 0.4153, 15488/20000\n",
      " Loss: 0.3773, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.465770 \n",
      "\n",
      "Epoch 48 \n",
      "--------------------------------\n",
      " Loss: 0.3801,   128/20000\n",
      " Loss: 0.3714,  2688/20000\n",
      " Loss: 0.3789,  5248/20000\n",
      " Loss: 0.3725,  7808/20000\n",
      " Loss: 0.3713, 10368/20000\n",
      " Loss: 0.4024, 12928/20000\n",
      " Loss: 0.3896, 15488/20000\n",
      " Loss: 0.3968, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.476254 \n",
      "\n",
      "Epoch 49 \n",
      "--------------------------------\n",
      " Loss: 0.3953,   128/20000\n",
      " Loss: 0.3899,  2688/20000\n",
      " Loss: 0.3462,  5248/20000\n",
      " Loss: 0.3647,  7808/20000\n",
      " Loss: 0.3923, 10368/20000\n",
      " Loss: 0.4258, 12928/20000\n",
      " Loss: 0.3656, 15488/20000\n",
      " Loss: 0.3970, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.487277 \n",
      "\n",
      "Epoch 50 \n",
      "--------------------------------\n",
      " Loss: 0.3498,   128/20000\n",
      " Loss: 0.4202,  2688/20000\n",
      " Loss: 0.3880,  5248/20000\n",
      " Loss: 0.3762,  7808/20000\n",
      " Loss: 0.3726, 10368/20000\n",
      " Loss: 0.3730, 12928/20000\n",
      " Loss: 0.4027, 15488/20000\n",
      " Loss: 0.3809, 18048/20000\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.479923 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "seed = 12 # 设置我们的全局随机种子\n",
    "\n",
    "model_type = \"LSTM\" # 指定要选择的模型[\"Transformer\", \"LSTM\"]\n",
    "# Transformer模型相关的超参数, 如果是LSTM则不需要关注\n",
    "embed_dim = 32\n",
    "num_heads = 2\n",
    "ff_dim = 32\n",
    "num_blocks = 2\n",
    "\n",
    "# LSTM模型相关的超参数\n",
    "lstm_embed = 32\n",
    "hidden_dim = 32\n",
    "\n",
    "ratio = 0.8 #训练集占数据集的比例\n",
    "batch_size = 128 # batch的大小\n",
    "epochs = 50 # 训练回合数\n",
    "learning_rate = 5e-4 #初始学习率的大小\n",
    "\n",
    "\n",
    "model = controller(seed,\n",
    "                   model_type,\n",
    "                   embed_dim,\n",
    "                   num_heads, \n",
    "                   ff_dim,\n",
    "                   num_blocks,\n",
    "                   lstm_embed,\n",
    "                   hidden_dim,\n",
    "                   ratio,\n",
    "                   batch_size,\n",
    "                   epochs, \n",
    "                   learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbe9992",
   "metadata": {
    "papermill": {
     "duration": 0.020142,
     "end_time": "2025-09-10T03:19:46.052421",
     "exception": false,
     "start_time": "2025-09-10T03:19:46.032279",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## `Evaluater` Part\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>这部分代码同学们不用修改调参，不会影响我们训练模型的性能。\n",
    "</div>\n",
    "\n",
    "这一部分我们调用已经训练好的模型，去预测我们`kaggle`给出的不带标签(`label`)的测试数据集，并将我们预测的结果写成一个`submission.csv`文件，放在指定的`kaggle`云文件夹下，便于自动评估我们提交模型的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d118df5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T03:19:46.095262Z",
     "iopub.status.busy": "2025-09-10T03:19:46.094487Z",
     "iopub.status.idle": "2025-09-10T03:19:46.456034Z",
     "shell.execute_reply": "2025-09-10T03:19:46.455183Z"
    },
    "papermill": {
     "duration": 0.384638,
     "end_time": "2025-09-10T03:19:46.457674",
     "exception": false,
     "start_time": "2025-09-10T03:19:46.073036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluater(model: nn.Module):\n",
    "    model.eval()\n",
    "    test_data_path = Path(\"/kaggle/input/lstm-transformers-2/processed_imdb_test_data.npz\")\n",
    "    submission_path = Path(\"/kaggle/working/submission.csv\")\n",
    "    with torch.no_grad():\n",
    "        test_data = np.load(test_data_path)\n",
    "        test_ids = test_data[\"ID\"]\n",
    "        test_tokens = test_data[\"x_test\"]\n",
    "        for i in range(5):\n",
    "            batch_test_tokens = torch.tensor(test_tokens[5000*i: 5000*(i+1), :], dtype=torch.long).cuda()\n",
    "            probs = model(batch_test_tokens)\n",
    "            _, batch_predicted_labels = torch.max(probs, dim=-1)\n",
    "            if i == 0:\n",
    "                predicted_labels = batch_predicted_labels.cpu().numpy()\n",
    "            else:\n",
    "                predicted_labels = np.hstack((predicted_labels, batch_predicted_labels.cpu().numpy()))\n",
    "\n",
    "    predicted_labels = pd.DataFrame(predicted_labels, index=test_ids, columns=[\"label\"])\n",
    "    predicted_labels.to_csv(submission_path, index=True, index_label=\"ID\")\n",
    "\n",
    "evaluater(model)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13470344,
     "sourceId": 113118,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 81.435157,
   "end_time": "2025-09-10T03:19:49.170038",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-10T03:18:27.734881",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
