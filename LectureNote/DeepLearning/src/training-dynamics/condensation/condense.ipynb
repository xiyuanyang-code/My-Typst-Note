{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络凝聚现象\n",
    "凝聚现象是指在神经网络的训练过程中，同一层神经元逐渐变得相似。例如，多个神经元最终可能对同一个输入产生非常相似的输出。尽管神经元一开始是随机初始化的，但通过训练，它们有趋于一致的倾向。这种现象揭示了神经网络内部的某种协调机制，有助于我们更好地理解神经网络的运作方式。我们将在这份代码中以全连接网络拟合一个一维函数的例子来说明这个现象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 相关文献\n",
    "[1] Tao Luo#, Zhi-Qin John Xu #, Zheng Ma, Yaoyu Zhang*, Phase diagram for two-layer ReLU neural networks at infinite-width limit, arxiv 2007.07497 (2020), Journal of Machine Learning Research (2021) [pdf](https://ins.sjtu.edu.cn/people/xuzhiqin/pub/phasediagram2020.pdf), and in [arxiv](https://arxiv.org/abs/2007.07497). \n",
    "\n",
    "[2] Hanxu Zhou, Qixuan Zhou, Tao Luo, Yaoyu Zhang*, Zhi-Qin John Xu*, Towards Understanding the Condensation of Neural Networks at Initial Training. arxiv 2105.11686 (2021) [pdf](https://ins.sjtu.edu.cn/people/xuzhiqin/pub/initial2105.11686.pdf), and in [arxiv](https://arxiv.org/abs/2105.11686), see slides and [video talk in Chinese](https://www.bilibili.com/video/BV1tb4y1d7CZ/?spm_id_from%253D333.999.0.0), NeurIPS2022. \n",
    "\n",
    "For more details, see [xuzhiqin condense](https://ins.sjtu.edu.cn/people/xuzhiqin/pubcondense.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在这里我们导入一些常用的库\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import re\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Condense \n",
    "![Condense](./pic/condense.png)\n",
    "这是一个理想的凝聚的示意图。在初始化时，各个神经元的输入权重差异很大，用不同的颜色表示。但是在经过一段时间训练后，中间隐藏神经元分成了两类，前两个神经元是一类，后三个神经元是另一类。在每一类中，不同神经元的输入权重是完全一样的~(颜色相同)，因此，它们的输出也是一样的。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   一些基础设定的参数\n",
    "注意要在ini_path中加入自己保存实验结果的地址。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch 1D dataset Training')\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument('--lr', default=0.00001, type=float, help='learning rate')  # 学习率\n",
    "parser.add_argument('--optimizer', default='adam', help='optimizer: sgd | adam')  # 优化器选择：sgd 或 adam\n",
    "parser.add_argument('--epochs', default=2000, type=int, metavar='N', help='number of total epochs to run')  # 总训练轮数\n",
    "parser.add_argument('--test_size', default=10000, type=int, help='the test size for model (default: 10000)')  # 测试集大小\n",
    "parser.add_argument('--save', default='trained_nets', help='path to save trained nets')  # 保存训练模型的路径\n",
    "parser.add_argument('--save_epoch', default=10, type=int, help='save every save_epochs')  # 每多少轮保存一次模型\n",
    "parser.add_argument('--rand_seed', default=0, type=int, help='seed for random num generator')  # 随机数生成器的种子\n",
    "parser.add_argument('--gamma', type=float, default=2, help='parameter initialization distribution variance power(We first assume that each layer is the same width.)')  # 参数初始化分布方差幂（假设每层宽度相同）\n",
    "parser.add_argument('--boundary', nargs='+', type=str, default=['-1', '1'], help='the boundary of 1D data')  # 一维数据的边界\n",
    "parser.add_argument('--training_size', default=4, type=int, help='the training size for model (default: 1000)')  # 训练集大小\n",
    "parser.add_argument('--act_func_name', default='Tanh', help='activation function')  # 激活函数名称\n",
    "parser.add_argument('--hidden_layers_width', nargs='+', type=int, default=[1000])  # 隐藏层宽度\n",
    "parser.add_argument('--input_dim', default=1, type=int, help='the input dimension for model (default: 1)')  # 模型输入维度\n",
    "parser.add_argument('--output_dim', default=1, type=int, help='the output dimension for model (default: 1)')  # 模型输出维度\n",
    "parser.add_argument('--device', default='cuda', type=str, help='device used to train (cpu or cuda)')  # 训练使用的设备（CPU 或 CUDA）\n",
    "parser.add_argument('--plot_epoch', default=100, type=int, help='step size of plotting interval (default: 1000)')  # 绘图间隔的步长\n",
    "parser.add_argument('--ini_path', default='***', type=str, help='the path to save experiment results')  # 保存实验结果的路径\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   实验结果的目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdirs(fn):  # Create directorys\n",
    "    if not os.path.isdir(fn):\n",
    "        os.makedirs(fn)\n",
    "    return fn\n",
    "\n",
    "\n",
    "def create_save_dir(path_ini): \n",
    "    subFolderName = re.sub(r'[^0-9]', '', str(datetime.datetime.now()))\n",
    "    path = os.path.join(path_ini, subFolderName)\n",
    "    mkdirs(path)\n",
    "    # mkdirs(os.path.join(path, 'output'))\n",
    "    return path\n",
    "\n",
    "\n",
    "args.path = create_save_dir(args.ini_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成用于训练和测试的数据\n",
    "我们使用的目标函数为：\n",
    "\\begin{equation}\n",
    "f(x)=0.2*\\mathrm{ReLU}(x-1/3) + 0.2*\\mathrm{ReLU} (-x-1/3)\n",
    "\\end{equation}\n",
    "其中数据的边界为$[-1,1]$。\n",
    "\n",
    "如果需要修改目标函数，可以在get_y函数中进行修改。在运行这一个代码块后会得到一个目标函数的图像，其中蓝色的点是训练数据，红色的线是测试数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(x):  # Function to fit\n",
    "    # y = torch.sin(x*math.pi)\n",
    "    y = 0.2*torch.relu(-1/3+x) + 0.2*torch.relu(-1/3-x)\n",
    "    return y\n",
    "\n",
    "for i in range(2):\n",
    "    if isinstance(args.boundary[i], str):\n",
    "        args.boundary[i] = eval(args.boundary[i])\n",
    "\n",
    "args.test_input = torch.reshape(torch.linspace(args.boundary[0], args.boundary[1], steps=args.test_size), [args.test_size, 1])\n",
    "\n",
    "\n",
    "args.training_input = torch.reshape(torch.linspace(args.boundary[0], args.boundary[1], steps=args.training_size), [args.training_size, 1])\n",
    "args.test_target = get_y(args.test_input)\n",
    "args.training_target = get_y(args.training_input)\n",
    "\n",
    "\n",
    "def plot_target(args):\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "\n",
    "    plt.plot(args.training_input.detach().cpu().numpy(),\n",
    "             args.training_target.detach().cpu().numpy(), 'b*', label='True')\n",
    "    plt.plot(args.test_input.detach().cpu().numpy(),\n",
    "             args.test_target.detach().cpu().numpy(), 'r-', label='Test')\n",
    "\n",
    "    ax.tick_params(labelsize=18)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_target(args)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络模型及其初始化\n",
    "\n",
    "首先，给定参数 $\\theta\\in \\mathbb{R}^M$，FNN函数 $f_{\\theta}(\\cdot)$ 是递归定义的。我们从输入层开始，定义 $f^{[0]}_{\\theta}(x)=x$，其中 $x\\in\\mathbb{R}^d$ 是输入数据。\n",
    "接下来，对于每一层 $l\\in[L-1]$，我们递归地定义 $f^{[l]}_{\\theta}$ 如下：\n",
    "$f^{[l]}{\\theta}(x)=\\sigma (W^{[l]} f^{[l-1]}{\\theta}(x)+b^{[l]})$\n",
    "这里，$\\sigma$ 是一个非线性激活函数，$W^{[l]}$ 是权重矩阵，$b^{[l]}$ 是偏置向量。\n",
    "最后，我们定义整个网络的输出为：\n",
    "\\begin{equation*}\n",
    "f_{\\theta}(x)=f(x,\\theta)=f^{[L]}{\\theta}(x)=W^{[L]} f^{[L-1]}{\\theta}(x)+b^{[L]}.\n",
    "\\end{equation*}\n",
    "注意，输出层通常不使用激活函数。\n",
    "现在，让我们来看看参数的初始化方法。我们使用带有参数 $\\gamma$ 的高斯分布来初始化参数，具体如下：\n",
    "\\begin{equation*}\n",
    "\\theta_{l} \\sim N(0, \\frac{1}{m_{l}^{\\gamma}}),\n",
    "\\end{equation*}\n",
    "这里，$\\theta_{l}$ 表示第 $l$ 层的参数，它是一个有序对 $\\theta_l=\\Big(W^{[l]},b^{[l]}\\Big),\\quad l\\in[L]$。$m_{l}$ 是第 $l$ 层的宽度（即神经元数量）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, gamma, hidden_layers_width=[100],  input_size=20, num_classes: int = 1000, act_layer: nn.Module = nn.ReLU()):\n",
    "        super(Linear, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers_width = hidden_layers_width\n",
    "        self.gamma = gamma\n",
    "        layers: List[nn.Module] = []\n",
    "        self.layers_width = [self.input_size]+self.hidden_layers_width\n",
    "        for i in range(len(self.layers_width)-1):\n",
    "            layers += [nn.Linear(self.layers_width[i],\n",
    "                                    self.layers_width[i+1]), act_layer]\n",
    "        layers += [nn.Linear(self.layers_width[-1], num_classes, bias=False)]\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self._initialize_weights()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self) -> None:\n",
    "\n",
    "        for obj in self.modules():\n",
    "            if isinstance(obj, (nn.Linear, nn.Conv2d)):\n",
    "                nn.init.normal_(obj.weight.data, 0, 1 /\n",
    "                                self.hidden_layers_width[0]**(self.gamma))\n",
    "                if obj.bias is not None:\n",
    "                    nn.init.normal_(obj.bias.data, 0, 1 /\n",
    "                                    self.hidden_layers_width[0]**(self.gamma))\n",
    "\n",
    "\n",
    "def get_act_func(act_func):\n",
    "    if act_func == 'Tanh':\n",
    "        return nn.Tanh()\n",
    "    elif act_func == 'ReLU':\n",
    "        return nn.ReLU()\n",
    "    elif act_func == 'Sigmoid':\n",
    "        return nn.Sigmoid()\n",
    "    else:\n",
    "        raise NameError('No such act func!')\n",
    "\n",
    "\n",
    "act_func = get_act_func(args.act_func_name)\n",
    "\n",
    "model = Linear(args.gamma, args.hidden_layers_width, args.input_dim,\n",
    "               args.output_dim, act_func).to(args.device)\n",
    "\n",
    "para_init = copy.deepcopy(model.state_dict())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义一步训练的函数\n",
    "\n",
    "在训练过程中，我们需要定义一个损失函数来衡量模型的表现。这里我们使用经验风险（empirical risk）作为我们的损失函数：\n",
    "\\begin{equation*}\n",
    "R_S(\\theta)=\\frac{1}{n}\\sum_{i=1}^n\\ell(f(x_i,\\theta),y(x_i)),\n",
    "\\end{equation*}\n",
    "这个公式中，$\\ell(\\cdot,\\cdot)$ 是一个可微的损失函数，用来衡量预测值 $f(x_i,\\theta)$ 和真实值 $y(x_i)$ 之间的差距。我们用 $\\nabla\\ell(y,y^*)$ 表示 $\\ell$ 对其第一个参数的导数。\n",
    "现在，我们的目标是最小化这个经验风险。我们可以使用梯度下降法来更新模型参数。对于一步梯度下降，参数更新公式如下：\n",
    "\\begin{equation*}\n",
    "\\theta_{t+1}=\\theta_t-\\eta\\nabla R_S(\\theta).\n",
    "\\end{equation*}\n",
    "这里，$\\theta_t$ 是当前的参数值，$\\theta_{t+1}$ 是更新后的参数值，$\\eta$ 是学习率（一个小的正数），$\\nabla R_S(\\theta)$ 是经验风险关于参数 $\\theta$ 的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_step(model, optimizer, loss_fn,  args):\n",
    "\n",
    "    model.train()\n",
    "    device = args.device\n",
    "    data, target = args.training_input.to(\n",
    "        device), args.training_target.to(device).to(torch.float)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(data)\n",
    "    loss = loss_fn(outputs, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义一步测试的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loss_fn, args):\n",
    "    model.eval()\n",
    "    device = args.device\n",
    "    with torch.no_grad():\n",
    "        data, target = args.test_input.to(\n",
    "            device), args.test_target.to(device).to(torch.float)\n",
    "        outputs = model(data)\n",
    "        loss = loss_fn(outputs, target)\n",
    "\n",
    "    return loss.item(), outputs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义可视化每一步损失的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(path, loss_train, x_log=False):\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    y2 = np.asarray(loss_train)\n",
    "    plt.plot(y2, 'k-', label='Train')\n",
    "    plt.xlabel('epoch', fontsize=18)\n",
    "    ax.tick_params(labelsize=18)\n",
    "    plt.yscale('log')\n",
    "    if x_log == False:\n",
    "        fntmp = os.path.join(path, 'loss.jpg')\n",
    "\n",
    "    else:\n",
    "        plt.xscale('log')\n",
    "        fntmp = os.path.join(path, 'loss_log.jpg')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fntmp,dpi=300)\n",
    "\n",
    "\n",
    "    plt.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义可视化神经网络输出的函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_output(path, args, output, epoch):\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "\n",
    "    plt.plot(args.training_input.detach().cpu().numpy(),\n",
    "             args.training_target.detach().cpu().numpy(), 'b*', label='True')\n",
    "    plt.plot(args.test_input.detach().cpu().numpy(),\n",
    "             output.detach().cpu().numpy(), 'r-', label='Test')\n",
    "\n",
    "    ax.tick_params(labelsize=18)\n",
    "    plt.legend(fontsize=18)\n",
    "    fn = mkdirs(os.path.join('%s'%path,'output'))\n",
    "    fntmp = os.path.join(fn, str(epoch)+'.jpg')\n",
    "\n",
    "    plt.savefig(fntmp, dpi=300)\n",
    "\n",
    "\n",
    "    plt.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在一个两层网络中定义神经元隐藏层的方向 （Orientation）和幅值 （Amplitude）\n",
    "\n",
    "一个两层的全连接网络的数学表达式如下：\n",
    "$$\n",
    "f_{\\boldsymbol{\\theta}}(\\boldsymbol{x})= \\sum_{k=1}^m a_k \\sigma\\left(\\boldsymbol{w}_k^{\\top} \\boldsymbol{x}\\right)\n",
    "$$\n",
    "在这个公式中，$\\boldsymbol{w}_k = (w_k,b_k)^T$ 是第 $k$ 个神经元的权重和偏置，$\\boldsymbol{x} =(x,1)^T$ 是输入向量（注意我们把偏置项并入了权重向量）。\n",
    "现在，让我们介绍两个重要的概念：\n",
    "\n",
    "1. 神经元的方向（orientation）：定义为 $\\frac{w_k}{|\\boldsymbol{w}_k|_2}$。这表示神经元在输入空间中的指向。\n",
    "2. 神经元的幅度（amplitude）：定义为 $|a_k|\\cdot|\\boldsymbol{w_k}|_2$。这反映了神经元对输出的影响程度。\n",
    "\n",
    "我们将($\\frac{w_k}{|\\boldsymbol{w}_k|_2}$,$|a_k|\\cdot|\\boldsymbol{w_k}|_2$)定义为神经元的特征 （feature）。\n",
    "\n",
    "接下来，我们来看看这个网络的参数初始化方法：\n",
    "$a_k^0\\sim \\mathcal{N}(0,\\beta_1^2),\\quad \\boldsymbol{w_k^0}\\sim\\mathcal{N}(0,\\beta_2^2\\boldsymbol{I}_d)$\n",
    "这里，$a_k^0$ 和 $\\boldsymbol{w_k^0}$ 分别是 $a_k$ 和 $\\boldsymbol{w_k}$ 的初始值。它们都是从高斯分布中随机采样的。一般的，$\\beta_1,\\beta_2$的大小为$m$的幂次，即为$m^{-t}$次。\n",
    "\n",
    "现在，我们引入两个重要的参数 $\\gamma$ 和 $\\gamma'$：\n",
    "\\begin{equation*}\n",
    "\\gamma = \\lim_{m\\rightarrow \\infty}-\\frac{\\log\\beta_1\\beta_2/\\alpha}{\\log m}, \\quad \\gamma'=\\lim_{m\\rightarrow \\infty} -\\frac{\\log\\beta_1/\\beta_2}{\\log m}\n",
    "\\end{equation*}\n",
    "这两个参数描述了初始化分布的方差（$\\beta_1$ 和 $\\beta_2$）如何随着网络宽度 $m$ 的增加而变化。它们对网络的训练动态有重要影响。\n",
    "特别地，当我们使用ReLU激活函数，即 $\\sigma(x)=\\mathrm{ReLU}(x)$ 时，我们可以得到一个相位图（phase diagram）。这个相位图展示了不同的 $\\gamma$ 和 $\\gamma'$ 值如何影响网络的行为。\n",
    "\n",
    "![phase](./pic/phase.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义函数可视化训练过程中神经元的特征\n",
    "\n",
    "1. get_ori_A函数是计算某一个检查点（checkpoint）中每个神经元的特征。\n",
    "2. get_ori_A_list函数是将不同checkpoint的神经元特征整合为一个列表。\n",
    "3. plot_feature函数是可视化神经元的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameter(checkpoint):\n",
    "    wei1 = checkpoint['features.0.weight']\n",
    "    bias = checkpoint['features.0.bias']\n",
    "    wei2 = checkpoint['features.2.weight']\n",
    "\n",
    "    return wei1, bias, wei2\n",
    "\n",
    "# To get the orientation and amplitude of each neuron in different layer.\n",
    "def get_ori_A(checkpoint):\n",
    "    wei1, bias, wei2 = get_parameter(checkpoint)\n",
    "\n",
    "    wei1 = wei1.squeeze()\n",
    "    bias = bias.squeeze()\n",
    "    wei2 = wei2.squeeze()\n",
    "    wei = wei1 / (wei1 ** 2 + bias ** 2)**(1/2)\n",
    "\n",
    "    bia = bias / (wei1 ** 2 + bias ** 2)**(1/2)\n",
    "    ori = torch.sign(bia) * torch.acos(wei)\n",
    "    A = wei2 * (wei1 ** 2 + bias ** 2)**(1/2)\n",
    "\n",
    "    return ori, A\n",
    "\n",
    "# get the orientation and amplitude of each neuron at the beginning and end of training.\n",
    "def get_ori_A_list(checkpoint_list):\n",
    "\n",
    "    if not isinstance(checkpoint_list, list):\n",
    "        ori, A=get_ori_A(checkpoint_list)\n",
    "        return [ori], [A]\n",
    "\n",
    "    else:\n",
    "        ori_ini, A_ini = get_ori_A(checkpoint_list[0])\n",
    "\n",
    "        ori, A = get_ori_A(checkpoint_list[1])\n",
    "\n",
    "        return [ori_ini, ori], [A_ini, A]\n",
    "    \n",
    "def plot_feature(path, ori, A, args, nota=''):\n",
    "    \"\"\"\n",
    "    It plots the feature of the input data\n",
    "    \n",
    "    :param path: the path to save the figure\n",
    "    :param ori: the feature orientation\n",
    "    :param A: the feature amplitude\n",
    "    :param args: the parameters of the model\n",
    "    \"\"\"\n",
    "\n",
    "    if args.input_dim == 1:\n",
    "\n",
    "        plt.figure()\n",
    "        ax = plt.gca()\n",
    "\n",
    "        if len(ori) == 1:\n",
    "\n",
    "            ori = ori[0].squeeze().detach().cpu().numpy()\n",
    "            A = A[0].squeeze().detach().cpu().numpy()\n",
    "\n",
    "        elif len(ori) == 2:\n",
    "            ori_ini, ori = ori[0].squeeze().detach().cpu(\n",
    "            ).numpy(), ori[1].squeeze().detach().cpu().numpy()\n",
    "            A_ini, A = A[0].squeeze().detach().cpu().numpy(\n",
    "            ), A[1].squeeze().detach().cpu().numpy()\n",
    "\n",
    "            print(ori_ini.shape, A_ini.shape)\n",
    "\n",
    "            plt.scatter(ori_ini, abs(A_ini), color='cyan', label='ini feature')\n",
    "\n",
    "        else:\n",
    "            raise Exception('The length of the checkpoint list is less than or equal to two.')\n",
    "\n",
    "        # mkdirs(r'%s\\\\feature' % (path))\n",
    "        fn = mkdirs(os.path.join('%s'%path,'feature'))\n",
    "        plt.scatter(ori, abs(A), color='r', label='fin feature')\n",
    "        plt.xlim(-3.16,3.16)\n",
    "        plt.xlabel('orientation', fontsize=18)\n",
    "        plt.ylabel('amplitude', fontsize=18)\n",
    "        plt.legend(fontsize=18)\n",
    "        # fntmp = r'%s\\\\feature\\\\%s' % (path, nota)\n",
    "        # fntmp = os.path.join(fn,'%s'%nota)\n",
    "        # save_fig(plt, fntmp, pdf=False, x_log=False, y_log=True)\n",
    "        plt.savefig(os.path.join(fn,'%s'%nota))\n",
    "        plt.close()\n",
    "    else:\n",
    "        raise Exception('Input dimension must equal to 1!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![out05](./999.jpg)![out1](./3999.jpg)![out2](./1999.jpg) -->\n",
    "<center class=\"half\">\n",
    "    <img src=\"./pic/999.jpg\" width=\"300\"/><img src=\"./pic/3999.jpg\" width=\"300\"/><img src=\"./pic/1999.jpg\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"./pic/999.png\" width=\"300\"/><img src=\"./pic/3999.png\" width=\"300\"/><img src=\"./pic/1999.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "三种不同大小的初始化下神经网络参数的预期结果。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进行训练的函数\n",
    "有了前面的关于神经网络的基础以及一些关于训练过程函数的定义之后，我们可以对神经网络进行训练，并可视化训练过程中我们关心的一些量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args.gamma = 1\n",
    "args.lr = 0.05\n",
    "args.epochs = 10000\n",
    "args.save_epoch = 1000\n",
    "args.plot_epoch = 1000\n",
    "args.optimizer = 'sgd'\n",
    "args.act_func_name = 'ReLU'\n",
    "args.savepath = os.path.join(args.path, 't=%s'%args.gamma)\n",
    "os.makedirs(args.savepath, exist_ok=True)\n",
    "act_func = get_act_func(args.act_func_name)\n",
    "\n",
    "model = Linear(args.gamma, args.hidden_layers_width, args.input_dim,\n",
    "               args.output_dim, act_func).to(args.device)\n",
    "\n",
    "para_init = copy.deepcopy(model.state_dict())\n",
    "if args.optimizer=='sgd':\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "else:\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "t0 = time.time()\n",
    "loss_training_lst=[]\n",
    "loss_test_lst = []\n",
    "for epoch in range(args.epochs+1):\n",
    "\n",
    "      model.train()\n",
    "      loss = train_one_step(\n",
    "        model, optimizer, loss_fn, args)\n",
    "      loss_test, output = test(\n",
    "          model, loss_fn, args)\n",
    "      loss_training_lst.append(loss)\n",
    "      loss_test_lst.append(loss_test)\n",
    "      if epoch % args.plot_epoch == 0:\n",
    "            print(\"[%d] loss: %.6f valloss: %.6f time: %.2f s\" %\n",
    "                  (epoch + 1, loss, loss_test, (time.time()-t0)))\n",
    "  \n",
    "      if (epoch+1) % (args.plot_epoch) == 0:\n",
    "          plot_loss(path=args.savepath,\n",
    "                    loss_train=loss_training_lst, x_log=True)\n",
    "          plot_loss(path=args.savepath,\n",
    "                    loss_train=loss_training_lst, x_log=False)\n",
    "\n",
    "          \n",
    "          para_now = copy.deepcopy(model.state_dict())\n",
    "          ori, A = get_ori_A_list([para_init,para_now])\n",
    "          plot_feature(args.savepath, ori, A, args,nota='%s'%epoch)\n",
    "          plot_model_output(args.savepath, args, output, epoch)\n",
    "          \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
